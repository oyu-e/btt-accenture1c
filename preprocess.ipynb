{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSWGac11ijuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization engine videos on YouTube:\n",
        "- https://www.youtube.com/watch?v=ZhVAjVraiRQ"
      ],
      "metadata": {
        "id": "WeYQkJ-ZqMYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for our code\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psk7MIUmfjmc",
        "outputId": "9cc17caf-2979-4eda-ae58-f67400d8d935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQJuMKl6lfle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea08a592-787a-4ab3-c119-d2d6179def73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# DO NOT RUN: This is used to connect Google Drive to add the files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_footer(text):\n",
        "  \"\"\"\n",
        "  Removes the footer using a combined regex pattern with  the lookahead assertion.\n",
        "\n",
        "  Args:\n",
        "      text: The text content.\n",
        "\n",
        "  Returns:\n",
        "      Text without the footer.\n",
        "  \"\"\"\n",
        "  combined_pattern = r\"\"\"\n",
        "    (  # Start of group\n",
        "      # ... (previous patterns for cookies, copyright, etc.) ...\n",
        "\n",
        "      |  # OR\n",
        "      About\\s*Us\\s*\\|?\\s*Contact\\s*Us\\s*\\|?\\s*FAQ\\s*\\|?\\s*Help  # Common page links\n",
        "      |  # OR\n",
        "      Terms\\s*and\\s*Conditions\\s*\\|?\\s*Privacy\\s*Statement  # Legal links\n",
        "      |  # OR\n",
        "      Cookie\\s*Policy\\s*\\|?\\s*Accessibility\\s*\\|?\\s*Sitemap  # More links\n",
        "      |  # OR\n",
        "      All\\s*Rights\\s*Reserved\\s*\\|?\\s*Copyright\\s*\\d{4}  # Copyright variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*is\\s*a\\s*(registered\\s*)?trademark\\s*of\\s*[a-zA-Z\\s]+\\.  # Trademark variations\n",
        "      |  # OR\n",
        "      Get\\s*in\\s*touch\\s*\\|?\\s*Send\\s*us\\s*a\\s*message  # Contact prompts\n",
        "      |  # OR\n",
        "      Back\\s*to\\s*top\\s*\\|?\\s*Top  # Back to top links\n",
        "      |  # OR\n",
        "      You\\s*are\\s*here:\\s*[\\w\\s\\/\\>]+  # Breadcrumb trails\n",
        "      |  # OR\n",
        "      (View\\s*|Edit)\\s*this\\s*page\\s*on\\s*[\\w\\s\\.]+  # Edit/view links\n",
        "      |  # OR\n",
        "      Last\\s*updated:\\s*\\d{1,2}\\s*[a-zA-Z]+\\s*\\d{4}  # Last updated info\n",
        "      |  # OR\n",
        "      Report\\s*a\\s*problem\\s*\\|?\\s*Suggest\\s*an\\s*edit  # Feedback options\n",
        "      |  # OR\n",
        "      Disclaimer:\\s*.*  # Disclaimers\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Sitemap  # Sitemap variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Terms\\s*and\\s*Conditions  # Terms and conditions variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Privacy\\s*Policy  # Privacy policy variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Contact\\s*Us  # Contact us variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*FAQ  # FAQ variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Help  # Help variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*About\\s*Us  # About us variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*About\\s*Author  # Author variations\n",
        "      |  # OR\n",
        "      [a-zA-Z\\s]+\\s*Conditions  # Conditions variations\n",
        "      |  # OR\n",
        "      (To\\s*read\\s*more\\s*on\\s*related\\s*topics|Enjoy\\s*extended\\s*coverage).*?  # Read more/extended coverage\n",
        "      |  # OR\n",
        "      (access\\s*subscription|Subscribe\\s*to\\s*our\\s*newsletter|Make\\s*a\\s*donation).*?  # Subscription/donation prompts\n",
        "      |  # OR\n",
        "      (Advertise\\s*with\\s*us|Advertising\\s*options|Become\\s*a\\s*sponsor).*?  # Advertising prompts\n",
        "      |  # OR\n",
        "      Design,\\s*CMS,\\s*Hosting\\s*&\\s*Web\\s*Development  # Website development info\n",
        "      |  # OR\n",
        "      (Subscribe\\s*now|Sign\\s*up\\s*for\\s*a\\s*(free|premium)\\s*subscription|Upgrade\\s*to\\s*premium|Get\\s*(full|unlimited)\\s*access).*?  # Subscription variations\n",
        "\n",
        "\n",
        "      # We can add more footer patterns here with \"|\" (OR)\n",
        "\n",
        "    )  # End of group\n",
        "    (?=\\s*$)  # Positive lookahead assertion for end of string\n",
        "  \"\"\"\n",
        "  cleaned_text = re.sub(combined_pattern, '', text, flags=re.IGNORECASE | re.DOTALL | re.VERBOSE)\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "ivs638aBQuhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to extract the main content from HTML\n",
        "def extract_main_content(html_text):\n",
        "    \"\"\"\n",
        "    Extracts the main content from HTML using BeautifulSoup.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "\n",
        "    # Attempting to extract main content by analyzing HTML tags\n",
        "    for selector in ['article', 'main', 'content', 'post']:\n",
        "        content = soup.find_all(selector)\n",
        "        if content:\n",
        "            return ' '.join([p.get_text() for p in content])\n",
        "\n",
        "    # Fallback: Extract text from paragraphs if specific tags are not found\n",
        "    return ' '.join([p.get_text() for p in soup.find_all('p')])"
      ],
      "metadata": {
        "id": "pWUB-YeV5lAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGLpriSAkqkh"
      },
      "outputs": [],
      "source": [
        "# Path to the shared folder, scraped from Timo\n",
        "folder_path = '/content/drive/MyDrive/BTT: Accenture 1C/scraped'\n",
        "# Path to the new folder with the corrected data\n",
        "dest_path = '/content/drive/MyDrive/BTT: Accenture 1C/cleaned'\n",
        "\n",
        "\n",
        "# Feel free to uncomment to view the contents of the folder\n",
        "#os.listdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_by_keywords(content, threshold):\n",
        "    \"\"\"\n",
        "    Filters articles based on tech-related keywords.\n",
        "\n",
        "    Args:\n",
        "        content: The text content of the article.\n",
        "        keywords: A list of tech-related keywords.\n",
        "        threshold: Minimum number of keyword matches to keep the article.\n",
        "\n",
        "    Returns:\n",
        "        Boolean indicating whether the article meets the keyword threshold.\n",
        "    \"\"\"\n",
        "    # print('Filtering by keywords...')\n",
        "\n",
        "    keywords = [\"technology\", \"ai\", \"software\", \"biotech\", \"startup\", \"tech\", \"machine learning\",\"investment\", \"engineering\", \"fintech\", \"artificial intelligence\", \"silicon valley\", \"financial\", \"infrastructure\", \"blockchain\"]\n",
        "\n",
        "    keyword_count = sum([content.count(keyword) for keyword in keywords])\n",
        "    return keyword_count >= threshold\n"
      ],
      "metadata": {
        "id": "PRfnhKpkvivt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def remove_irrelevant_sections(text):\n",
        "    \"\"\"\n",
        "    Removes irrelevant sections from the article text.\n",
        "    \"\"\"\n",
        "    # Define irrelevant patterns\n",
        "    irrelevant_patterns = [\n",
        "        r'copyright.*', r'cookie settings.*', r'donation.*', r'subscribe.*',\n",
        "        r'©.*', r'privacy policy.*', r'forgot username.*', r'forgot password.*',\n",
        "        r'user agreement.*', r'contact us.*', r'earn commission.*', r'affiliate link.*',\n",
        "        r'click.*', r'subscribe.*', r'visit.*', r'read.*', r'article.*'\n",
        "    ]\n",
        "\n",
        "    # Remove irrelevant content based on patterns\n",
        "    for pattern in irrelevant_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    # Clean up any residual irrelevant text\n",
        "    text = re.sub(r'\\n+', '\\n', text)  # Remove excessive newlines\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "S_iUp0sIgB2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_error_messages(text):\n",
        "    # print('Removing error messages...')\n",
        "    error_patterns = [\n",
        "    r'404 not found', r'403 forbidden', r'500 internal server error',\n",
        "    r'502 Bad Gateway', r'503 Service Unavailable', r'504 Gateway Timeout',\n",
        "    r'400 Bad Request', r'401 Unauthorized', r'408 Request Timeout',\n",
        "    r'Page not found', r'Access denied', r'Try again later',\n",
        "    r'This content is available to subscribers only', r'Sign in to read more',\n",
        "    r'Your connection is not private', r'Under maintenance',\n",
        "    r'Connection refused', r'Oops, something went wrong'\n",
        "]\n",
        "\n",
        "    for pattern in error_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return None  # Skip this article\n",
        "    return text"
      ],
      "metadata": {
        "id": "ePGuBOfzfFoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_short_articles(text, min_word_count=150):\n",
        "    # print('Removing short articles...')\n",
        "    words = word_tokenize(text)\n",
        "    return text if len(words) >= min_word_count else None"
      ],
      "metadata": {
        "id": "GGMA6EEjgFh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the provided text into words.\n",
        "\n",
        "    Args:\n",
        "        text: The text to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        List of tokenized words.\n",
        "    \"\"\"\n",
        "    # print(\"Tokenizing text...\")\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "zsHzwA7kgIHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qSxD-gStqAiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contractions (e.g., \"don't\", \"isn't\") will be expanded to their full forms for consistency before applying lemmatization\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n"
      ],
      "metadata": {
        "id": "bSzCa3O3_GEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Lemmatizes the provided tokens using WordNetLemmatizer.\n",
        "\n",
        "    Args:\n",
        "        tokens: List of tokenized words.\n",
        "\n",
        "    Returns:\n",
        "        List of lemmatized words.\n",
        "    \"\"\"\n",
        "    # print(\"Lemmatizing tokens...\")\n",
        "    # tokens = expand_contractions(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "35LUvc7friHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    # print(\"Normalizing text...\")\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "wJYCHtyTgKYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_article(article_path):\n",
        "    # print(f\"Cleaning article: {article_path}\")\n",
        "    with open(article_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    if not text:\n",
        "        print(1)\n",
        "        return None\n",
        "\n",
        "\n",
        "    text = expand_contractions(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    if not filter_short_articles(text):\n",
        "        print(2)\n",
        "        return None\n",
        "\n",
        "    if filter_by_keywords(text, 1) is None:\n",
        "        print(3)\n",
        "        return None\n",
        "\n",
        "    text = remove_error_messages(text)\n",
        "    if text is None:\n",
        "        print(4)\n",
        "        return None\n",
        "\n",
        "    text = remove_footer(text)\n",
        "\n",
        "\n",
        "    # Apply tokenization and normalization\n",
        "    tokens = tokenize_text(text)\n",
        "    normalized_tokens = normalize_text(' '.join(tokens))\n",
        "    lemmatized_tokens = lemmatize_tokens(normalized_tokens)\n",
        "    return ' '.join(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "e4eO7NjAgM73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files(input_folder, dest_folder):\n",
        "    if not os.path.exists(dest_folder):\n",
        "        os.makedirs(dest_folder)\n",
        "\n",
        "    cleaned_data = []\n",
        "    rejected_data = []\n",
        "    count = 1\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if count > 50:\n",
        "            break  # Only process the first 50 files\n",
        "\n",
        "        if not filename.endswith('.txt'):\n",
        "            continue  # Skip non-text files\n",
        "\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Clean the article\n",
        "        cleaned_text = clean_article(file_path)\n",
        "        if cleaned_text:\n",
        "            cleaned_data.append([filename, cleaned_text, content])\n",
        "        else:\n",
        "            # If cleaning returns None, log the rejection\n",
        "            rejected_data.append([filename, content, \"Rejected\"])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Save cleaned data to a CSV\n",
        "    df_cleaned = pd.DataFrame(cleaned_data, columns=['Filename', 'Cleaned Text', 'Original Text'])\n",
        "    df_cleaned.to_csv(os.path.join(dest_folder, 'cleaned_articles.csv'), index=False)\n",
        "\n",
        "    # Save rejected files to another CSV\n",
        "    df_rejected = pd.DataFrame(rejected_data, columns=['Filename', 'Content', 'Reason'])\n",
        "    df_rejected.to_csv(os.path.join(dest_folder, 'rejected_articles.csv'), index=False)"
      ],
      "metadata": {
        "id": "4H0YHeV5gPrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = '/content/drive/MyDrive/BTT: Accenture 1c/scraped'\n",
        "dest_folder = '/content/drive/MyDrive/BTT: Accenture 1C/cleaned'\n",
        "process_files(input_folder, dest_folder)"
      ],
      "metadata": {
        "id": "GOY8B03sgaFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "fa4744dd-4516-4ccf-8cdb-b85c08a451b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/BTT: Accenture 1c/scraped'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-19cdddc933aa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/BTT: Accenture 1c/scraped'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdest_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/BTT: Accenture 1C/cleaned'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ccb38e948fc7>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(input_folder, dest_folder)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Only process the first 50 files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/BTT: Accenture 1c/scraped'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yxi0G_7V8Cfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting entities to identify key players, companies, products, or organizations in the industry\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Apply to the DataFrame\n"
      ],
      "metadata": {
        "id": "UrDSs45ND1Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking URLS for tech domains and tech words. if you want it to just look for domains, delete tech\n",
        "tech_sources = ['gizmodo.com','cnn.com/tech','cnet.com','tech'] #common tech domains and tech words\n",
        "def is_tech_source(url):\n",
        "  return any(source in url for source in tech_sources)\n",
        "example_url = 'https://www.cnn.com/business/tech'\n",
        "result = is_tech_source(example_url)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "DUlihqvlBNTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2061ebac-30c5-44e6-a712-6848efac56e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testcase_1 = \"The Week 17 NFL schedule opens with Malik Willis and the Tennessee Titans hosting Dak Prescott and the Dallas Cowboys on Thursday Night Football. All-Pro running back Derrick Henry (hip) is doubtful and Josh Dobbs is expected to make his first start of the season in place of Ryan Tannehill and Malik Willis for Tennessee. Before locking in any Titans vs. Cowboys Showdown NFL DFS lineups for Thursday Night Football on sites like DraftKings and FanDuel, you NEED to see what SportsLine's Jimmie Kaylor has to say. Kaylor is a NFL and DFS expert for SportsLine, who opened the 2022 NFL season by winning DraftKings' NFL Showdown Thursday Kickoff Millionaire contest. He edged out over 355,000 other entrants in the contest and also has multiple five-figure tournament wins on his DFS resume. Kaylor, who has been cashing big all season, has covered the NFL and college football for close to a decade as a member of the Pro Football Writers of America, and his background as a former college and NFL player gives him a unique perspective when building his fantasy lineups and locking in his betting picks.   Kaylor has been spot-on all season, hitting on epic NFL DFS picks like Jerry Jeudy (4-102-1), Stefon Diggs (12-148-3, 6-106-1, 7-92-1), Amari Cooper (7-101-1, 5-131-1), Nick Chubb (23-113-1, 23-101-2), CeeDee Lamb (8-87-1), Deebo Samuel (6-115-1) Cooper Kupp (14-122), Travis Kelce (7-25-4, 10-106), Davante Adams (3-124-2), Justin Fields (179-1-1, 82-1), Derrick Henry (132-2), Jimmy Garoppolo (228-4), Michael Pittman (7-61-1), Christian McCaffrey (26-108-1, 6-30), Kirk Cousins (460-4), Justin Jefferson (12-123-1), J.K. Dobbins (13-125), Aaron Jones (17-90, 4-36-1), and T.J. Hockenson (13-109-2). Anyone who has followed his picks is WAY UP! Now, Kaylor has turned his attention to Cowboys vs. Titans on Week 17 Thursday Night Football and locked in his top daily Fantasy football picks. We can tell you that one of Kaylor's top NFL DFS picks for the Titans vs. Cowboys TNF matchup is Dallas wide receiver CeeDee Lamb. 'On paper this matchup looks outstanding for CeeDee Lamb and the Dallas passing attack. Tennessee has one of the best rushing defenses in the NFL, but has been highly susceptible against the pass. Lamb has been on a tear, hauling in 17 passes for 246 yards and two touchdowns in his last two games. My biggest concern is that Dallas will jump out to a big early lead with Tennessee being decimated by injuries. Still, I expect Lamb to be active early and notch his third consecutive 100-yard receiving performance.' Kaylor is also targeting another under-the-radar option for TNF who could post HUGE numbers and carry your lineups to victory on Thursday Night Football. This pick could be the difference between winning your tournaments and cash games or going home with nothing! You ABSOLUTELY need to see who it is before locking in any lineups.  Be sure to follow Jimmie (@jimmiekaylor) on Twitter for notifications about rosters changes/updates.   What are the top NFL DFS picks for Titans vs. Cowboys on FanDuel and DraftKings? And which player is a MUST-ROSTER? ... Join SportsLine now to see NFL expert and DraftKings Millionaire winner Jimmie Kaylor's top picks, stacks, and player pools for FanDuel and DraftKings, and cash in BIG on NFL DFS! Share This Story  2024 Detroit Lions futures picks: Breaking down win totals, Super Bowl odds, schedule, depth chart and more Florida State vs. Georgia Tech prediction, odds, spread, line, start time: Proven expert releases college football picks, best bets, props for College Football Classic 2024 MLB pitching props: Robbie Ray among expert's best bets for Tuesday, August 20 2024 Chicago Bears futures picks: Breaking down win totals, Super Bowl odds, schedule, depth chart and more\"\n",
        "clean_article('/content/drive/MyDrive/BTT: Accenture 1C/scraped/5454832.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "wVMefUf0KITT",
        "outputId": "c38dceca-a340-4993-b094-21b32bddcaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/BTT: Accenture 1C/scraped/5454832.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-69df93c6c03a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestcase_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The Week 17 NFL schedule opens with Malik Willis and the Tennessee Titans hosting Dak Prescott and the Dallas Cowboys on Thursday Night Football. All-Pro running back Derrick Henry (hip) is doubtful and Josh Dobbs is expected to make his first start of the season in place of Ryan Tannehill and Malik Willis for Tennessee. Before locking in any Titans vs. Cowboys Showdown NFL DFS lineups for Thursday Night Football on sites like DraftKings and FanDuel, you NEED to see what SportsLine's Jimmie Kaylor has to say. Kaylor is a NFL and DFS expert for SportsLine, who opened the 2022 NFL season by winning DraftKings' NFL Showdown Thursday Kickoff Millionaire contest. He edged out over 355,000 other entrants in the contest and also has multiple five-figure tournament wins on his DFS resume. Kaylor, who has been cashing big all season, has covered the NFL and college football for close to a decade as a member of the Pro Football Writers of America, and his background as a former college and NFL player gives him a unique perspective when building his fantasy lineups and locking in his betting picks.   Kaylor has been spot-on all season, hitting on epic NFL DFS picks like Jerry Jeudy (4-102-1), Stefon Diggs (12-148-3, 6-106-1, 7-92-1), Amari Cooper (7-101-1, 5-131-1), Nick Chubb (23-113-1, 23-101-2), CeeDee Lamb (8-87-1), Deebo Samuel (6-115-1) Cooper Kupp (14-122), Travis Kelce (7-25-4, 10-106), Davante Adams (3-124-2), Justin...\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/BTT: Accenture 1C/scraped/5454832.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-5af7bb0c1763>\u001b[0m in \u001b[0;36mclean_article\u001b[0;34m(article_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# print(f\"Cleaning article: {article_path}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/BTT: Accenture 1C/scraped/5454832.txt'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}